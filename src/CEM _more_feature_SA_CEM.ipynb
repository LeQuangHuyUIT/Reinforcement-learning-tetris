{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip to the last cell to run the best player found by SA_CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import heapq\n",
    "from numpy.linalg import cholesky\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tetris_utils import *\n",
    "from tetris import *\n",
    "from copy import deepcopy\n",
    "import pygame,sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tetris import TetrisApp\n",
    "App=TetrisApp()\n",
    "from tetris_reinforcement_learner import TetrisReinforcementLearner\n",
    "RL_learner=TetrisReinforcementLearner(App)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "done = False\n",
    "batch_size = 40\n",
    "\n",
    "\n",
    "state_size = 270\n",
    "action_size = 30\n",
    "# agent.load(\"./save/cartpole-dqn.h5\")\n",
    "App.init_game()\n",
    "def pixalize_stone(stone):\n",
    "    container=np.zeros((2,10))\n",
    "    if len(stone)==1:\n",
    "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
    "    elif stone[0][0]==7:\n",
    "        container[0][...,4:4+len(stone[0])]=stone[0]\n",
    "        container[1][...,4:4+len(stone[0])]=stone[1]\n",
    "    else:\n",
    "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
    "        container[1][...,3:3+len(stone[0])]=stone[1]\n",
    "    return container\n",
    "    \n",
    "def preprocess_data(state):\n",
    "    \n",
    "    board=np.array(state['board'])\n",
    "    stone=state['stone']\n",
    "    next_stone=state['next_stone']\n",
    "    pixal_stone=pixalize_stone(stone)\n",
    "    pixal_next_stone=pixalize_stone(next_stone)\n",
    "\n",
    "    final_board=np.append(pixal_next_stone,pixal_stone,axis=0)\n",
    "    final_board=np.append(final_board,board,axis=0)\n",
    "    final_board=np.where(final_board>0,1,0)\n",
    "#     print(final_board)\n",
    "    final_board=final_board.flatten()\n",
    "    final_board=final_board.reshape(1,270)\n",
    "    return final_board\n",
    "\n",
    "\n",
    "def update_new(reinforcement_learner, old_state,new_state):## Taking holes into account\n",
    "    change_in_pile_height = reinforcement_learner.get_pile_height(new_state) - reinforcement_learner.get_pile_height(old_state)\n",
    "    reward = -1 * change_in_pile_height\n",
    "    return reward# container=np.zeros((len(stone),10))\n",
    "# for i in range(len(stone)):\n",
    "#     container[i][...,3:3+len(stone[0])]=stone[i]\n",
    "# board=board.flatten()\n",
    "# board=board[:,np.newaxis]\n",
    "# board = np.where(board>0,1,0)\n",
    "# stone_idx=find_stone_idx(stone)\n",
    "# next_stone_idx=find_stone_idx(state['next_stone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(state):\n",
    "    height=RL_learner.get_pile_height(state)\n",
    "    holes=RL_learner.get_holes(state)\n",
    "    contours=RL_learner.get_contours(state)\n",
    "    row_holes=RL_learner.get_row_holes(state)\n",
    "    features=np.array([[height],[holes],[contours],[row_holes]])\n",
    "    return features\n",
    "def get_current_state_CEM_value(state,action_sequence):\n",
    "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
    "    features=get_feature(successor_state)\n",
    "    for feature, value in features.items():\n",
    "        CEM_val+=RL_learner.CEM_weights[feature]*value\n",
    "        \n",
    "def get_following_state_CEM_value(state,action_sequence,cur_theta):\n",
    "    \n",
    "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
    "    next_state_reward=RL_learner.get_next_action_reward(state,action_sequence)\n",
    "#     print('next_state_reward is',next_state_reward)\n",
    "    done=successor_state['gameover']\n",
    "    if not done:\n",
    "        \n",
    "        features=get_feature(successor_state)\n",
    "        features=np.append(features,np.array([[next_state_reward]]),axis=0)\n",
    "    return np.squeeze(cur_theta.T.dot(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_theta(mu,sigma,num_sample):\n",
    "    R = cholesky(sigma)\n",
    "    s = np.dot(np.random.randn(num_sample, 5), R) + mu.T\n",
    "    return s\n",
    "\n",
    "statistic=[]\n",
    "statistic1=[]\n",
    "def mysoftmaxchoice(x):\n",
    "    expx=np.exp(x)\n",
    "    expx=expx/max(expx)\n",
    "    smx=expx/sum(expx)\n",
    "    return np.random.choice(len(x), size=None, replace=True, p=np.array(smx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2089d2cf436f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mlegal_action_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRL_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_legal_action_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mactions\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlegal_action_sequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0mnext_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_following_state_CEM_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcur_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mnext_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlegal_action_sequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mApp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_action_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-28b3b51d5992>\u001b[0m in \u001b[0;36mget_following_state_CEM_value\u001b[1;34m(state, action_sequence, cur_theta)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuccessor_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state_reward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_theta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#top_5_theta = deque(maxlen=5)\n",
    "mu0=np.zeros((5,1)) #initialize mu\n",
    "sigma0=np.eye(5,5) #initialize theta\n",
    "num_sample=50\n",
    "s=get_sample_theta(mu0,100*sigma0,num_sample)\n",
    "num_repeat=1\n",
    "\n",
    "theta_grade=[] # store the grade each theta earned\n",
    "for theta_idx in range(num_sample):\n",
    "    R_temp=0\n",
    "    print(theta_idx,end=\"\\r\")\n",
    "    cur_theta=np.array(s[theta_idx])\n",
    "            #cur_theta=cur_theta[:,np.newaxis]\n",
    "    for i in range(num_repeat):\n",
    "        App.init_game()\n",
    "        if not App.pygame_initted: App.init_pygame()\n",
    "        App.gameover=False\n",
    "        App.paused=False\n",
    "        App.start_game()\n",
    "        done=False\n",
    "        while not done:\n",
    "            App.display_board()\n",
    "            state=RL_learner.capture_state_attributes(App)\n",
    "            dont_burn_my_cpu=pygame.time.Clock()  \n",
    "\n",
    "            ## calculate the next state value, which is store in the list: next_val\n",
    "            next_val=[] # store the next_state_value\n",
    "            legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "            for actions in legal_action_sequences:\n",
    "                next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "            next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "            App.play_action_sequence(next_action)\n",
    "            new_state=RL_learner.capture_state_attributes(App)\n",
    "            done=new_state['gameover']\n",
    "        \n",
    "        if done:\n",
    "            R_temp=R_temp+App.lines\n",
    "                \n",
    "    theta_grade.append(R_temp/num_repeat)\n",
    "#top_5_theta_idx=list(map(theta_grade.index, heapq.nlargest(5, theta_grade)))\n",
    "#for idx in top_5_theta_idx:\n",
    "#    top_5_theta.append(s[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_theta=s\n",
    "top_50_grade=theta_grade\n",
    "#top_50_theta_idx=list(map(theta_grade.tolist().index, heapq.nlargest(50, theta_grade)))\n",
    "#for idx in top_50_theta_idx:\n",
    "#    top_50_theta.append(s[idx])\n",
    "#    top_50_grade.append(theta_grade[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85.0, 0.0, 6.98], [288.0, 0.0, 20.0]]\n",
      "iter:  1 , idx:  25\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a20c87c09068>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mlegal_action_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRL_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_legal_action_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mactions\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlegal_action_sequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                     \u001b[0mnext_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_following_state_CEM_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcur_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m                 \u001b[0mnext_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlegal_action_sequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mApp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_action_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-28b3b51d5992>\u001b[0m in \u001b[0;36mget_following_state_CEM_value\u001b[1;34m(state, action_sequence, cur_theta)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuccessor_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state_reward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_theta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#statistic1=np.loadtxt(\"statistics.txt\")\n",
    "#statistic1=statistic1.tolist()\n",
    "#top_50_grade=np.loadtxt(\"theta_grade.txt\")\n",
    "#top_50_theta=np.loadtxt(\"theta.txt\")\n",
    "\n",
    "#num_sample=100\n",
    "T0=.15\n",
    "num_repeat=1\n",
    "for i in range(12):\n",
    "    np_top_50_theta=np.array(top_50_theta).T\n",
    "    mu=np.mean(np_top_50_theta,axis=1)\n",
    "    sigma=np.cov(np_top_50_theta)+np.eye(5)*1e-16#+max(5-(i+1)/10,0)*np.eye(3)\n",
    "    \n",
    "    s_cem=get_sample_theta(mu,sigma,50)\n",
    "    \n",
    "    T=T0-i/100\n",
    "    ds=20*T*get_sample_theta(mu0,sigma0,50)\n",
    "    \n",
    "    s_sa=top_50_theta+ds\n",
    "    #s=np.vstack((np_top_50_theta+ds,s50))\n",
    "    #theta_grade=[] # store the grade each theta earned\n",
    "    #candidate=np.zeros((5,0))\n",
    "    #candidate_grade=[]\n",
    "    #top_50_theta=[]\n",
    "    #top_50_grade=[]\n",
    "    for theta_idx in range(50):\n",
    "        R_temp=0\n",
    "        print('iter: ',i,', idx: ',theta_idx,end=\"\\r\")\n",
    "        for j in range(num_repeat):\n",
    "            \n",
    "            App.init_game()\n",
    "            if not App.pygame_initted: App.init_pygame()\n",
    "            App.gameover=False\n",
    "            App.paused=False\n",
    "            App.start_game()\n",
    "            done=False\n",
    "            cur_theta=s_sa[theta_idx]\n",
    "            #cur_theta=cur_theta[:,np.newaxis]\n",
    "            while not done:\n",
    "                App.display_board()\n",
    "                state=RL_learner.capture_state_attributes(App)\n",
    "                dont_burn_my_cpu=pygame.time.Clock()  \n",
    "\n",
    "                ## calculate the next state value, which is store in the list: next_val\n",
    "                next_val=[] # store the next_state_value\n",
    "                legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "                for actions in legal_action_sequences:\n",
    "                    next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "                next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "                App.play_action_sequence(next_action)\n",
    "                new_state=RL_learner.capture_state_attributes(App)\n",
    "                done=new_state['gameover']\n",
    "            \n",
    "            if done:\n",
    "                R_temp=R_temp+App.lines\n",
    "        \n",
    "        R_temp=R_temp/num_repeat\n",
    "        deltaR=(R_temp+1e-16)/(top_50_grade[theta_idx]+1e-16)-1\n",
    "        if deltaR>0:\n",
    "            top_50_theta[theta_idx]=cur_theta\n",
    "            top_50_grade[theta_idx]=R_temp\n",
    "        else:\n",
    "            pa=np.exp(deltaR/T)\n",
    "            if np.random.choice([True, False], size=None, replace=True, p=[pa, 1-pa]):\n",
    "                top_50_theta[theta_idx]=cur_theta\n",
    "                top_50_grade[theta_idx]=R_temp\n",
    "            #else:\n",
    "            #    candidate=np.vstack((candidate,cur_theta))\n",
    "            #    candidate_grade.append(R_temp)\n",
    "    \n",
    "    #cem_theta=[]\n",
    "    cem_grade=np.zeros(50)\n",
    "    for theta_idx in range(50):\n",
    "        R_temp=0\n",
    "        print('iter: ',i,', idx: ',theta_idx+50,end=\"\\r\")\n",
    "        for j in range(num_repeat):\n",
    "            \n",
    "            App.init_game()\n",
    "            if not App.pygame_initted: App.init_pygame()\n",
    "            App.gameover=False\n",
    "            App.paused=False\n",
    "            App.start_game()\n",
    "            done=False\n",
    "            cur_theta=s_cem[theta_idx]\n",
    "            #cur_theta=cur_theta[:,np.newaxis]\n",
    "            while not done:\n",
    "                App.display_board()\n",
    "                state=RL_learner.capture_state_attributes(App)\n",
    "                dont_burn_my_cpu=pygame.time.Clock()  \n",
    "\n",
    "                ## calculate the next state value, which is store in the list: next_val\n",
    "                next_val=[] # store the next_state_value\n",
    "                legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "                for actions in legal_action_sequences:\n",
    "                    next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "                next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "                App.play_action_sequence(next_action)\n",
    "                new_state=RL_learner.capture_state_attributes(App)\n",
    "                done=new_state['gameover']\n",
    "            \n",
    "            if done:\n",
    "                R_temp=R_temp+App.lines\n",
    "        cem_grade[theta_idx]=R_temp/num_repeat\n",
    "    \n",
    "    s=np.vstack((top_50_theta,s_cem))\n",
    "\n",
    "    theta_grade_array=np.hstack((top_50_grade,cem_grade))\n",
    "    theta_grade=theta_grade_array.tolist()\n",
    "            \n",
    "    top_50_theta_idx=list(map(theta_grade.index, heapq.nlargest(50, theta_grade)))\n",
    "    \n",
    "    top_50_theta=s[top_50_theta_idx]\n",
    "    top_50_grade=theta_grade_array[top_50_theta_idx]\n",
    "      \n",
    "    statistic1.append([np.max(theta_grade),np.min(theta_grade),np.mean(theta_grade)])\n",
    "    #print(i)\n",
    "    print(statistic1)\n",
    "    statistic_np=np.array(statistic1)\n",
    "    np.savetxt(\"statistics.txt\",statistic_np)\n",
    "    theta_grade_np=np.array(theta_grade)\n",
    "    np.savetxt(\"theta_grade.txt\",top_50_grade)\n",
    "    theta_np=np.array(top_50_theta)\n",
    "    np.savetxt(\"theta.txt\",theta_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best test\n",
    "best_theta=[4.231203317932727970e+00, -2.039093680914587736e+01, -4.425357456410593748e+00, 8.171306159156509707e+00, 8.939371296855847859e+00\n",
    "]\n",
    "num_repeat=10\n",
    "R_temp=0\n",
    "T0=.01\n",
    "for j in range(num_repeat):\n",
    "    App.init_game()\n",
    "    if not App.pygame_initted: App.init_pygame()\n",
    "    App.gameover=False\n",
    "    App.paused=False\n",
    "    App.start_game()\n",
    "    done=False\n",
    "    cur_theta=np.array(best_theta)\n",
    "    #cur_theta=cur_theta[:,np.newaxis]\n",
    "    while not done:\n",
    "        App.display_board()\n",
    "        state=RL_learner.capture_state_attributes(App)\n",
    "        dont_burn_my_cpu=pygame.time.Clock()  \n",
    "\n",
    "        ## calculate the next state value, which is store in the list: next_val\n",
    "        next_val=[] # store the next_state_value\n",
    "        legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "        for actions in legal_action_sequences:\n",
    "            next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "        next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "        App.play_action_sequence(next_action)\n",
    "        new_state=RL_learner.capture_state_attributes(App)\n",
    "        done=new_state['gameover']\n",
    "\n",
    "    if done:\n",
    "        R_temp=R_temp+App.lines\n",
    "R_ave=R_temp/num_repeat\n",
    "print(R_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
