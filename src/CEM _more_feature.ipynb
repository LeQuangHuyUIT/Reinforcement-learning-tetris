{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import heapq\n",
    "from numpy.linalg import cholesky\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tetris_utils import *\n",
    "from tetris_reinforcement_learner import TetrisReinforcementLearner\n",
    "from tetris import *\n",
    "from copy import deepcopy\n",
    "import pygame,sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tetris import TetrisApp\n",
    "from tetris_reinforcement_learner import TetrisReinforcementLearner\n",
    "App=TetrisApp()\n",
    "RL_learner=TetrisReinforcementLearner(App)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want, you can skip to the last cell to see the performance :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_learner=TetrisReinforcementLearner(App)\n",
    "App=TetrisApp()\n",
    "done = False\n",
    "batch_size = 40\n",
    "\n",
    "\n",
    "state_size = 270\n",
    "action_size = 30\n",
    "# agent.load(\"./save/cartpole-dqn.h5\")\n",
    "App.init_game()\n",
    "def pixalize_stone(stone):\n",
    "    container=np.zeros((2,10))\n",
    "    if len(stone)==1:\n",
    "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
    "    elif stone[0][0]==7:\n",
    "        container[0][...,4:4+len(stone[0])]=stone[0]\n",
    "        container[1][...,4:4+len(stone[0])]=stone[1]\n",
    "    else:\n",
    "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
    "        container[1][...,3:3+len(stone[0])]=stone[1]\n",
    "    return container\n",
    "    \n",
    "def preprocess_data(state):\n",
    "    \n",
    "    board=np.array(state['board'])\n",
    "    stone=state['stone']\n",
    "    next_stone=state['next_stone']\n",
    "    pixal_stone=pixalize_stone(stone)\n",
    "    pixal_next_stone=pixalize_stone(next_stone)\n",
    "\n",
    "    final_board=np.append(pixal_next_stone,pixal_stone,axis=0)\n",
    "    final_board=np.append(final_board,board,axis=0)\n",
    "    final_board=np.where(final_board>0,1,0)\n",
    "#     print(final_board)\n",
    "    final_board=final_board.flatten()\n",
    "    final_board=final_board.reshape(1,270)\n",
    "    return final_board\n",
    "\n",
    "\n",
    "def update_new(reinforcement_learner, old_state,new_state):## Taking holes into account\n",
    "    change_in_pile_height = reinforcement_learner.get_pile_height(new_state) - reinforcement_learner.get_pile_height(old_state)\n",
    "    reward = -1 * change_in_pile_height\n",
    "    return reward# container=np.zeros((len(stone),10))\n",
    "# for i in range(len(stone)):\n",
    "#     container[i][...,3:3+len(stone[0])]=stone[i]\n",
    "# board=board.flatten()\n",
    "# board=board[:,np.newaxis]\n",
    "# board = np.where(board>0,1,0)\n",
    "# stone_idx=find_stone_idx(stone)\n",
    "# next_stone_idx=find_stone_idx(state['next_stone'])\n",
    "\n",
    "def get_feature(state):\n",
    "    height=RL_learner.get_pile_height(state)\n",
    "    holes=RL_learner.get_holes(state)\n",
    "    contours=RL_learner.get_contours(state)\n",
    "    row_holes=RL_learner.get_row_holes(state)\n",
    "    features=np.array([[height],[holes],[contours],[row_holes]])\n",
    "    return features\n",
    "def get_current_state_CEM_value(state,action_sequence):\n",
    "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
    "    features=get_feature(successor_state)\n",
    "    for feature, value in features.items():\n",
    "        CEM_val+=RL_learner.CEM_weights[feature]*value\n",
    "        \n",
    "def get_following_state_CEM_value(state,action_sequence,cur_theta):\n",
    "    \n",
    "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
    "    next_state_reward=RL_learner.get_next_action_reward(state,action_sequence)\n",
    "#     print('next_state_reward is',next_state_reward)\n",
    "    done=successor_state['gameover']\n",
    "    if not done:\n",
    "        \n",
    "        features=get_feature(successor_state)\n",
    "        features=np.append(features,np.array([[next_state_reward]]),axis=0)\n",
    "    return cur_theta.T.dot(features)\n",
    "\n",
    "def get_sample_theta(mu,sigma,num_sample):\n",
    "    R = cholesky(sigma)\n",
    "    s = np.dot(np.random.randn(num_sample, 5), R) + mu.T\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize stuff, which has been done. The latest theta is store in the 'top_15_theta_75samples.txt', file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you want to search the theta from the beginning, you can uncomment the following code, and then run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_15_theta = deque(maxlen=15)\n",
    "# mu=np.zeros((5,1)) #initialize mu\n",
    "# sigma=100*np.eye(5,5) #initialize theta\n",
    "# s=get_sample_theta(mu,sigma,100)\n",
    "# num_sample=50\n",
    "# theta_grade=[] # store the grade each theta earned\n",
    "# for theta_idx in range(num_sample):\n",
    "\n",
    "#     App.init_game()\n",
    "#     if not App.pygame_initted: App.init_pygame()\n",
    "#     App.gameover=False\n",
    "#     App.paused=False\n",
    "#     App.start_game()\n",
    "#     done=False\n",
    "#     while not done:\n",
    "#         App.display_board()\n",
    "#         state=RL_learner.capture_state_attributes(App)\n",
    "#         dont_burn_my_cpu=pygame.time.Clock()  \n",
    "\n",
    "#         cur_theta=np.array(s[theta_idx])\n",
    "#         cur_theta=cur_theta[:,np.newaxis]\n",
    "\n",
    "#         ## calculate the next state value, which is store in the list: next_val\n",
    "#         next_val=[] # store the next_state_value\n",
    "#         legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "#         for actions in legal_action_sequences:\n",
    "#             next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "#         next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "#         App.play_action_sequence(next_action)\n",
    "#         new_state=RL_learner.capture_state_attributes(App)\n",
    "#         done=new_state['gameover']\n",
    "#         if done:\n",
    "#             theta_grade.append(App.lines)\n",
    "\n",
    "# top_15_theta_idx=list(map(theta_grade.index, heapq.nlargest(15, theta_grade)))\n",
    "# for idx in top_15_theta_idx:\n",
    "#     top_15_theta.append(s[idx])\n",
    "# statistic1.append([np.max(theta_grade),np.min(theta_grade),np.mean(theta_grade)])\n",
    "# print(statistic1)\n",
    "# statistic_np=np.array(statistic1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the latest prameter theta from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_15_load_theta= np.loadtxt('top_15_theta_75samples.txt')\n",
    "statistic1=[]\n",
    "top_15_theta = deque(maxlen=15)\n",
    "top_15_theta=top_15_load_theta\n",
    "top_15_theta=top_15_theta.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10000):\n",
    "    np_top_15_theta=np.array(top_15_theta).T\n",
    "    mu=np.mean(np_top_15_theta,axis=1)\n",
    "    sigma=np.cov(np_top_15_theta)+np.eye(5)*1e-16#+max(5-(i+1)/10,0)*np.eye(3)\n",
    "    s=get_sample_theta(mu,sigma,100)\n",
    "\n",
    "    num_sample=75\n",
    "    theta_grade=[] # store the grade each theta earned\n",
    "    for theta_idx in range(num_sample):\n",
    "        print('the theta_idx is', theta_idx)\n",
    "        App.init_game()\n",
    "        if not App.pygame_initted: App.init_pygame()\n",
    "        App.gameover=False\n",
    "        App.paused=False\n",
    "        App.start_game()\n",
    "        done=False\n",
    "        cur_theta=np.array(s[theta_idx])\n",
    "        cur_theta=cur_theta[:,np.newaxis]\n",
    "        while not done:\n",
    "            App.display_board()\n",
    "            state=RL_learner.capture_state_attributes(App)\n",
    "            dont_burn_my_cpu=pygame.time.Clock()  \n",
    "            ## calculate the next state value, which is store in the list: next_val\n",
    "            next_val=[] # store the next_state_value\n",
    "            legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "            for actions in legal_action_sequences:\n",
    "                next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "            next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "            App.play_action_sequence(next_action)\n",
    "            new_state=RL_learner.capture_state_attributes(App)\n",
    "            done=new_state['gameover']\n",
    "            if done:\n",
    "                theta_grade.append(App.lines)\n",
    "\n",
    "    top_15_theta_idx=list(map(theta_grade.index, heapq.nlargest(15, theta_grade)))\n",
    "    for idx in top_15_theta_idx:\n",
    "        top_15_theta.append(s[idx])\n",
    "        \n",
    "    statistic1.append([np.max(theta_grade),np.min(theta_grade),np.mean(theta_grade)])\n",
    "#     print(i)\n",
    "#     print(statistic1)\n",
    "    statistic_np=np.array(statistic1)\n",
    "    np.savetxt(\"statistics_of_5_feature_without_noise.txt\",statistic_np)\n",
    "    distribution=np.array(top_15_theta)\n",
    "    np.savetxt(\"top_15_theta_75samples.txt\",distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here is the best theta we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theta:  [[-0.3645792 ,\n",
    "        -28.55589636,  \n",
    "        -5.72603727,  \n",
    "        -1.3392655 ,\n",
    "        4.34739603]] which leads to 4397 line cleared\n",
    "        \n",
    "theta: [[0.19267331,\n",
    "       -27.34949633, \n",
    "       -5.62961297,  \n",
    "       -1.33885247,\n",
    "       4.34739604]] which leads to 9044 line cleared\n",
    "                \n",
    "statistic              \n",
    "[[4397, 15, 829.8666666666667],\n",
    " [4397, 15, 829.8666666666667],\n",
    " [9044, 18, 789.9866666666667]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can try out the following  code to see the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_theta=np.array([0.19267331,\n",
    "       -27.34949633, \n",
    "       -5.62961297,  \n",
    "       -1.33885247,\n",
    "       4.34739604])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-39794b8cbc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlegal_action_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRL_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legal_action_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlegal_action_sequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mnext_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_following_state_CEM_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mnext_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlegal_action_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mApp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_action_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bef6a746e926>\u001b[0m in \u001b[0;36mget_following_state_CEM_value\u001b[0;34m(state, action_sequence, cur_theta)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_following_state_CEM_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msuccessor_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRL_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_successor_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mnext_state_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRL_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_action_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#     print('next_state_reward is',next_state_reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cs_8803_ACR/hw2/tetris_reinforcement_learning_ai-20190319T185403Z-001/tetris_reinforcement_learning_ai/tetris_reinforcement_learner.py\u001b[0m in \u001b[0;36mget_successor_state\u001b[0;34m(self, state, action_sequence)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_successor_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0msuccessor_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction_sequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cs_8803_ACR/hw2/tetris_reinforcement_learning_ai-20190319T185403Z-001/tetris_reinforcement_learning_ai/tetris_reinforcement_learner.py\u001b[0m in \u001b[0;36mcopy_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         return {\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0;34m\"board\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"board\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0;34m\"stone\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;34m\"next_stone\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"next_stone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/[work]/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/[work]/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/[work]/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/[work]/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/[work]/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nil\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \"\"\"Deep copy operation on arbitrary Python objects.\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    App.init_game()\n",
    "    if not App.pygame_initted: App.init_pygame()\n",
    "    App.gameover=False\n",
    "    App.paused=False\n",
    "    App.start_game()\n",
    "    done=False\n",
    "    cur_theta=np.array(cur_theta)\n",
    "    cur_theta=cur_theta[:,np.newaxis]\n",
    "    while not done:\n",
    "        App.display_board()\n",
    "        state=RL_learner.capture_state_attributes(App)\n",
    "        dont_burn_my_cpu=pygame.time.Clock()  \n",
    "        ## calculate the next state value, which is store in the list: next_val\n",
    "        next_val=[] # store the next_state_value\n",
    "        legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "        for actions in legal_action_sequences:\n",
    "            next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "        next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "        App.play_action_sequence(next_action)\n",
    "        new_state=RL_learner.capture_state_attributes(App)\n",
    "        done=new_state['gameover']\n",
    "        if done:\n",
    "            theta_grade.append(App.lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
