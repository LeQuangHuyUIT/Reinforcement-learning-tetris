{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import heapq\n",
    "from numpy.linalg import cholesky\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tetris_utils import *\n",
    "from tetris_reinforcement_learner import TetrisReinforcementLearner\n",
    "from tetris import *\n",
    "from copy import deepcopy\n",
    "import pygame,sys\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tetris import TetrisApp\n",
    "App=TetrisApp()\n",
    "from tetris_reinforcement_learner import TetrisReinforcementLearner\n",
    "RL_learner=TetrisReinforcementLearner(App)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "done = False\n",
    "batch_size = 40\n",
    "\n",
    "\n",
    "state_size = 270\n",
    "action_size = 30\n",
    "# agent.load(\"./save/cartpole-dqn.h5\")\n",
    "App.init_game()\n",
    "def pixalize_stone(stone):\n",
    "    container=np.zeros((2,10))\n",
    "    if len(stone)==1:\n",
    "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
    "    elif stone[0][0]==7:\n",
    "        container[0][...,4:4+len(stone[0])]=stone[0]\n",
    "        container[1][...,4:4+len(stone[0])]=stone[1]\n",
    "    else:\n",
    "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
    "        container[1][...,3:3+len(stone[0])]=stone[1]\n",
    "    return container\n",
    "    \n",
    "def preprocess_data(state):\n",
    "    \n",
    "    board=np.array(state['board'])\n",
    "    stone=state['stone']\n",
    "    next_stone=state['next_stone']\n",
    "    pixal_stone=pixalize_stone(stone)\n",
    "    pixal_next_stone=pixalize_stone(next_stone)\n",
    "\n",
    "    final_board=np.append(pixal_next_stone,pixal_stone,axis=0)\n",
    "    final_board=np.append(final_board,board,axis=0)\n",
    "    final_board=np.where(final_board>0,1,0)\n",
    "#     print(final_board)\n",
    "    final_board=final_board.flatten()\n",
    "    final_board=final_board.reshape(1,270)\n",
    "    return final_board\n",
    "\n",
    "\n",
    "def update_new(reinforcement_learner, old_state,new_state):## Taking holes into account\n",
    "    change_in_pile_height = reinforcement_learner.get_pile_height(new_state) - reinforcement_learner.get_pile_height(old_state)\n",
    "    reward = -1 * change_in_pile_height\n",
    "    return reward# container=np.zeros((len(stone),10))\n",
    "# for i in range(len(stone)):\n",
    "#     container[i][...,3:3+len(stone[0])]=stone[i]\n",
    "# board=board.flatten()\n",
    "# board=board[:,np.newaxis]\n",
    "# board = np.where(board>0,1,0)\n",
    "# stone_idx=find_stone_idx(stone)\n",
    "# next_stone_idx=find_stone_idx(state['next_stone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(state):\n",
    "    height=RL_learner.get_pile_height(state)\n",
    "    holes=RL_learner.get_holes(state)\n",
    "    contours=RL_learner.get_contours(state)\n",
    "    features=np.array([[height],[holes],[contours]])\n",
    "    return features\n",
    "def get_current_state_CEM_value(state,action_sequence):\n",
    "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
    "    features=get_feature(successor_state)\n",
    "    for feature, value in features.items():\n",
    "        CEM_val+=RL_learner.CEM_weights[feature]*value\n",
    "        \n",
    "def get_following_state_CEM_value(state,action_sequence,cur_theta):\n",
    "    \n",
    "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
    "    done=successor_state['gameover']\n",
    "    if not done:\n",
    "        \n",
    "        features=get_feature(successor_state)\n",
    "\n",
    "    return cur_theta.T.dot(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_theta(mu,sigma,num_sample):\n",
    "    R = cholesky(sigma)\n",
    "    s = np.dot(np.random.randn(num_sample, 3), R) + mu.T\n",
    "    return s\n",
    "\n",
    "statistic=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial search for the theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_5_theta = deque(maxlen=5)\n",
    "mu=np.zeros((3,1)) #initialize mu\n",
    "sigma=100*np.eye(3,3)+5 #initialize theta\n",
    "s=get_sample_theta(mu,sigma,100)\n",
    "\n",
    "num_sample=100\n",
    "theta_grade=[] # store the grade each theta earned\n",
    "for theta_idx in range(num_sample):\n",
    "\n",
    "    App.init_game()\n",
    "    if not App.pygame_initted: App.init_pygame()\n",
    "    App.gameover=False\n",
    "    App.paused=False\n",
    "    App.start_game()\n",
    "    done=False\n",
    "    while not done:\n",
    "        App.display_board()\n",
    "        state=RL_learner.capture_state_attributes(App)\n",
    "        dont_burn_my_cpu=pygame.time.Clock()  \n",
    "\n",
    "        cur_theta=np.array(s[theta_idx])\n",
    "        cur_theta=cur_theta[:,np.newaxis]\n",
    "\n",
    "        ## calculate the next state value, which is store in the list: next_val\n",
    "        next_val=[] # store the next_state_value\n",
    "        legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "        for actions in legal_action_sequences:\n",
    "            next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "        next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "        App.play_action_sequence(next_action)\n",
    "        new_state=RL_learner.capture_state_attributes(App)\n",
    "        done=new_state['gameover']\n",
    "        if done:\n",
    "            theta_grade.append(App.lines)\n",
    "\n",
    "top_5_theta_idx=list(map(theta_grade.index, heapq.nlargest(5, theta_grade)))\n",
    "for idx in top_5_theta_idx:\n",
    "    top_5_theta.append(s[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt(\"top_five_theta_for_CEM_3_features.txt\",top_5_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_load_theta= np.loadtxt('top_five_theta_for_CEM_3_features.txt')\n",
    "statistic1=[]\n",
    "top_5_theta = deque(maxlen=15)\n",
    "top_5_theta=top_5_load_theta\n",
    "top_5_theta=top_5_theta.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    np_top_5_theta=np.array(top_5_theta).T\n",
    "    mu=np.mean(np_top_5_theta,axis=1)\n",
    "    sigma=np.cov(np_top_5_theta)+max(5-(i+1)/10,0)*np.eye(3)\n",
    "    s=get_sample_theta(mu,sigma,50)\n",
    "\n",
    "    num_sample=50\n",
    "    theta_grade=[] # store the grade each theta earned\n",
    "    for theta_idx in range(num_sample):\n",
    "        print('iter: ',i,', idx: ',theta_idx,end=\"\\r\")\n",
    "        App.init_game()\n",
    "        if not App.pygame_initted: App.init_pygame()\n",
    "        App.gameover=False\n",
    "        App.paused=False\n",
    "        App.start_game()\n",
    "        done=False\n",
    "        while not done:\n",
    "            App.display_board()\n",
    "            state=RL_learner.capture_state_attributes(App)\n",
    "            dont_burn_my_cpu=pygame.time.Clock()  \n",
    "\n",
    "            cur_theta=np.array(s[theta_idx])\n",
    "            cur_theta=cur_theta[:,np.newaxis]\n",
    "\n",
    "            ## calculate the next state value, which is store in the list: next_val\n",
    "            next_val=[] # store the next_state_value\n",
    "            legal_action_sequences=RL_learner.get_legal_action_sequences(state)\n",
    "            for actions in legal_action_sequences:\n",
    "                next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
    "            next_action=legal_action_sequences[np.argmax(next_val)]\n",
    "            App.play_action_sequence(next_action)\n",
    "            new_state=RL_learner.capture_state_attributes(App)\n",
    "            done=new_state['gameover']\n",
    "            if done:\n",
    "                theta_grade.append(App.lines)\n",
    "\n",
    "    top_5_theta_idx=list(map(theta_grade.index, heapq.nlargest(5, theta_grade)))\n",
    "    for idx in top_5_theta_idx:\n",
    "        top_5_theta.append(s[idx])\n",
    "        \n",
    "    statistic1.append([np.max(theta_grade),np.min(theta_grade),np.mean(theta_grade)])\n",
    "    print(i)\n",
    "    print(statistic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[355, 5, 10],\n",
       " [1455, 5, 255],\n",
       " [2119, 8, 387.67],\n",
       " [3036, 7, 549.47],\n",
       " [4638, 3, 676.15],\n",
       " [3499, 38, 807.28],\n",
       " [2729, 225, 1446.6],\n",
       " [2493, 186, 758.25],\n",
       " [2000, 152, 618.875],\n",
       " [2021, 248, 1010.25],\n",
       " [2152, 114, 864.125],\n",
       " [2428, 86, 863.875],\n",
       " [1718, 25, 666.0],\n",
       " [2167, 95, 862.875],\n",
       " [2196, 200, 795.75],\n",
       " [2288, 82, 634.0],\n",
       " [1661, 101, 851.625],\n",
       " [2108, 434, 1062.0]]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic_for_non_noise=statistic\n",
    "statistic_for_non_noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
