{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMA-ES-Tetris.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#git clone"
      ],
      "metadata": {
        "id": "q72mybvFpCXj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoT78_y6o0EA",
        "outputId": "4d9bcacd-3ce2-4342-8997-b972e0a5d5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Reinforcement-learning-tetris'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Total 42 (delta 0), reused 0 (delta 0), pack-reused 42\u001b[K\n",
            "Unpacking objects: 100% (42/42), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LeQuangHuyUIT/Reinforcement-learning-tetris"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMfcgR-fpHEX",
        "outputId": "ae5c5b95-3084-4eea-dac5-4a013c9720e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygame\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 1.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Reinforcement-learning-tetris/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDFfAn2qRFw",
        "outputId": "72fa1cbf-4e8c-4e2f-c434-bb7b69fd81d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Reinforcement-learning-tetris/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import lib"
      ],
      "metadata": {
        "id": "b9oKFYMpqZ8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import heapq\n",
        "from numpy.linalg import cholesky\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tetris_utils import *\n",
        "from tetris_reinforcement_learner import TetrisReinforcementLearner\n",
        "from tetris import *\n",
        "from copy import deepcopy\n",
        "import pygame,sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tetris import TetrisApp\n",
        "from tetris_reinforcement_learner import TetrisReinforcementLearner\n",
        "import math"
      ],
      "metadata": {
        "id": "LABykrcqpO14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec2801d-2fb5-4281-a354-1c3d3426c35f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.0 (SDL 2.0.16, Python 3.7.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "nhAGjFGMqqye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "App=TetrisApp()\n",
        "RL_learner=TetrisReinforcementLearner(App)\n",
        "done = False\n",
        "batch_size = 40\n",
        "\n",
        "\n",
        "state_size = 270\n",
        "action_size = 30\n",
        "# agent.load(\"./save/cartpole-dqn.h5\")\n",
        "App.init_game()\n",
        "def pixalize_stone(stone):\n",
        "    container=np.zeros((2,10))\n",
        "    if len(stone)==1:\n",
        "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
        "    elif stone[0][0]==7:\n",
        "        container[0][...,4:4+len(stone[0])]=stone[0]\n",
        "        container[1][...,4:4+len(stone[0])]=stone[1]\n",
        "    else:\n",
        "        container[0][...,3:3+len(stone[0])]=stone[0]\n",
        "        container[1][...,3:3+len(stone[0])]=stone[1]\n",
        "    return container\n",
        "    \n",
        "def preprocess_data(state):\n",
        "    \n",
        "    board=np.array(state['board'])\n",
        "    stone=state['stone']\n",
        "    next_stone=state['next_stone']\n",
        "    pixal_stone=pixalize_stone(stone)\n",
        "    pixal_next_stone=pixalize_stone(next_stone)\n",
        "\n",
        "    final_board=np.append(pixal_next_stone,pixal_stone,axis=0)\n",
        "    final_board=np.append(final_board,board,axis=0)\n",
        "    final_board=np.where(final_board>0,1,0)\n",
        "#     print(final_board)\n",
        "    final_board=final_board.flatten()\n",
        "    final_board=final_board.reshape(1,270)\n",
        "    return final_board\n",
        "\n",
        "\n",
        "def update_new(reinforcement_learner, old_state,new_state):## Taking holes into account\n",
        "    change_in_pile_height = reinforcement_learner.get_pile_height(new_state) - reinforcement_learner.get_pile_height(old_state)\n",
        "    reward = -1 * change_in_pile_height\n",
        "    return reward\n",
        "\n",
        "def get_feature(state):\n",
        "    height=RL_learner.get_pile_height(state)\n",
        "    holes=RL_learner.get_holes(state)\n",
        "    contours=RL_learner.get_contours(state)\n",
        "    row_holes=RL_learner.get_row_holes(state)\n",
        "    features=np.array([[height],[holes],[contours],[row_holes]])\n",
        "    return features\n",
        "    \n",
        "def get_current_state_CEM_value(state,action_sequence):\n",
        "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
        "    features=get_feature(successor_state)\n",
        "    for feature, value in features.items():\n",
        "        CEM_val+=RL_learner.CEM_weights[feature]*value\n",
        "        \n",
        "def get_following_state_CEM_value(state,action_sequence,cur_theta):\n",
        "    \n",
        "    successor_state=RL_learner.get_successor_state(state,action_sequence)\n",
        "    next_state_reward=RL_learner.get_next_action_reward(state,action_sequence)\n",
        "#     print('next_state_reward is',next_state_reward)\n",
        "    done=successor_state['gameover']\n",
        "    if not done:\n",
        "        \n",
        "        features=get_feature(successor_state)\n",
        "        features=np.append(features,np.array([[next_state_reward]]),axis=0)\n",
        "    return cur_theta.T.dot(features)\n",
        "\n",
        "def get_sample_theta(mu,sigma,num_sample):\n",
        "    R = cholesky(sigma)\n",
        "    s = np.dot(np.random.randn(num_sample, 5), R) + mu.T\n",
        "    return s"
      ],
      "metadata": {
        "id": "uTzs8oRmqqQq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(cur_theta):\n",
        "    App.init_game()\n",
        "    # if not App.pygame_initted: App.init_pygame()\n",
        "    App.gameover=False\n",
        "    App.paused=False\n",
        "    App.start_game()\n",
        "    done=False\n",
        "\n",
        "    cur_theta=np.array(cur_theta)\n",
        "    cur_theta=cur_theta[:,np.newaxis]\n",
        "\n",
        "    while not done:\n",
        "        state=RL_learner.capture_state_attributes(App)\n",
        "\n",
        "        ## calculate the next state value, which is store in the list: next_val\n",
        "        next_val=[] # store the next_state_value\n",
        "        legal_action_sequences = RL_learner.get_legal_action_sequences(state)\n",
        "\n",
        "        for actions in legal_action_sequences:\n",
        "            next_val.append(get_following_state_CEM_value(state,actions,cur_theta))\n",
        "        next_action=legal_action_sequences[np.argmax(next_val)]\n",
        "        \n",
        "        App.play_action_sequence(next_action)\n",
        "        new_state=RL_learner.capture_state_attributes(App)\n",
        "        done=new_state['gameover']\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return App.lines"
      ],
      "metadata": {
        "id": "fxdv28F0Qtrh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_mu(weights, Z):\n",
        "    return sum([w * z for w, z in zip(weights, Z)])"
      ],
      "metadata": {
        "id": "Blrq7lRFtMNq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code CMA-ES\n"
      ],
      "metadata": {
        "id": "oHBkZjfEzI5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import floor, log, sqrt\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn"
      ],
      "metadata": {
        "id": "V3BiqrBJzK8h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expect_multivariate_norm(N):\n",
        "    return N ** 0.5 * (1 - 1 / (4 * N) + 1 / (21 * N ** 2))\n",
        "    \n",
        "def sample(n, sigma, mean, B, D):\n",
        "    features = mean.size(0)\n",
        "    z = torch.randn(features, n, device=mean.device, dtype=mean.dtype)\n",
        "    s = mean.view(-1, 1) + sigma * B.matmul(D.matmul(z))\n",
        "    return s.T, z.T"
      ],
      "metadata": {
        "id": "baoJL2zR5OOL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_f(s):\n",
        "    thetas = s.detach().cpu().numpy()\n",
        "    return [evaluate(theta) for theta in thetas]"
      ],
      "metadata": {
        "id": "Qo3zQ8D6DVBG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CMA_ES():\n",
        "    def __init__(self, N, step_mode='auto', step_decay=None, initial_step_size=None, samples=None, oversample=0.0):\n",
        "        self.N = N\n",
        "        self.recommended_steps = range(1, floor(1e3 * N ** 2))\n",
        "\n",
        "        # selection settings\n",
        "        self.samples = 4 + floor(3 * log(N)) if samples is None else samples\n",
        "        self.oversample = oversample\n",
        "        self.mu = self.samples / 2\n",
        "        self.weights = torch.tensor([log(self.mu + 0.5)]) - torch.linspace(start=1, end=self.mu,\n",
        "                                                                           steps=floor(self.mu)).log()\n",
        "        self.weights = self.weights / self.weights.sum()\n",
        "        self.weights = self.weights / self.weights.sum()\n",
        "        self.mu = floor(self.mu)\n",
        "        self.mueff = (self.weights.sum() ** 2 / (self.weights ** 2).sum()).item()\n",
        "\n",
        "        # adaptation settings\n",
        "        self.cc = (4 + self.mueff / N) / (N + 4 + 2 * self.mueff / N)\n",
        "        self.cs = (self.mueff + 2) / (N + self.mueff + 5)\n",
        "        self.c1 = 2 / ((self.N + 1.3) ** 2 + self.mueff)\n",
        "        self.cmu = 2 * (self.mueff - 2 + 1 / self.mueff) / ((N + 2) ** 2 + 2 * self.mueff / 2)\n",
        "        self.damps = 1 + 2 * max(0.0, sqrt((self.mueff - 1.0) / (N + 1)) - 1) + self.cs\n",
        "        self.chiN = expect_multivariate_norm(N)\n",
        "        self.step_size = 0.5 if initial_step_size is None else initial_step_size\n",
        "        self.step_mode = step_mode\n",
        "        if step_mode == 'decay' and step_decay is None:\n",
        "            raise Exception('decay mode requires you set a step decay')\n",
        "        self.step_decay = (1.0 - step_decay) if step_decay is not None else None\n",
        "\n",
        "        # variables\n",
        "        self.mean = torch.zeros(N)\n",
        "        self.b = torch.eye(N)\n",
        "        self.d = torch.eye(N)\n",
        "        self.c = torch.matmul(self.b.matmul(self.d), self.b.matmul(self.d).T)  # c = B D D B.T\n",
        "\n",
        "        self.pc = torch.zeros(N)\n",
        "        self.ps = torch.zeros(N)\n",
        "        self.gen_count = 1\n",
        "\n",
        "    def step(self, objective_f):\n",
        "        s, z = sample(self.samples + floor(self.oversample), self.step_size, self.mean, self.b, self.d)\n",
        "        self.oversample = self.oversample * 0.8\n",
        "\n",
        "        # print(s, z)\n",
        "        f = objective_f(s)\n",
        "        results = [{'parameters': s[i], 'z': z[i], 'fitness': f} for i, f in enumerate(f)]\n",
        "\n",
        "        ranked_results = sorted(results, key=lambda x: x['fitness'], reverse=True)\n",
        "\n",
        "        selected_results = ranked_results[0:self.mu]\n",
        "        z = torch.stack([g['z'] for g in selected_results])\n",
        "        g = torch.stack([g['parameters'] for g in selected_results])\n",
        "\n",
        "        self.mean = (g * self.weights.unsqueeze(1)).sum(0)\n",
        "        zmean = (z * self.weights.unsqueeze(1)).sum(0)\n",
        "\n",
        "         # step size\n",
        "        self.ps = (1 - self.cs) * self.ps + sqrt(self.cs * (2.0 - self.cs)) * self.b.matmul(zmean)\n",
        "\n",
        "        correlation = self.ps.norm() / self.chiN\n",
        "\n",
        "        # delay the introduction of the rank 1 update\n",
        "        denominator = sqrt(1 - (1 - self.cs) ** (2 * self.gen_count / self.samples))\n",
        "        threshold = 1.4e2 / self.N + 1\n",
        "        hsig = correlation / denominator < threshold\n",
        "        hsig = 1.0 if hsig else 0.0\n",
        "\n",
        "        # adapt step size\n",
        "        if self.step_mode == 'auto':\n",
        "            self.step_size = self.step_size * ((self.cs / self.damps) * (correlation - 1.0)).exp()\n",
        "        elif self.step_mode == 'nodamp':\n",
        "            self.step_size = self.step_size * (correlation - 1.0).exp()\n",
        "        elif self.step_mode == 'decay':\n",
        "            self.step_size = self.step_size * self.step_decay\n",
        "        elif self.step_mode == 'constant':\n",
        "            pass\n",
        "        else:\n",
        "            raise Exception('step_mode must be auto | nodamp | decay | constant')\n",
        "\n",
        "        # a mind bending way to write a exponential smoothed moving average\n",
        "        # zmean does not contain step size or mean, so allows us to add together\n",
        "        # updates of different step sizes\n",
        "        self.pc = (1 - self.cc) * self.pc + hsig * sqrt(self.cc * (2.0 - self.cc) * self.mueff) * self.b.matmul(\n",
        "            self.d).matmul(zmean)\n",
        "        # which we then combine to make a covariance matrix, from 1 (mean) datapoint!\n",
        "        # this is why it's called \"rank 1\" update\n",
        "        pc_cov = self.pc.unsqueeze(1).matmul(self.pc.unsqueeze(1).t())\n",
        "        # mix back in the old covariance if hsig == 0\n",
        "        pc_cov = pc_cov + (1 - hsig) * self.cc * (2 - self.cc) * self.c\n",
        "\n",
        "        # estimate cov for all selected samples (weighted by rank)\n",
        "        bdz = self.b.matmul(self.d).matmul(z.t())\n",
        "        cmu_cov = torch.matmul(bdz, self.weights.diag_embed())\n",
        "        cmu_cov = cmu_cov.matmul(bdz.t())\n",
        "\n",
        "        self.c = (1.0 - self.c1 - self.cmu) * self.c + (self.c1 * pc_cov) + (self.cmu * cmu_cov)\n",
        "\n",
        "        # pull out the eigenthings and do the business\n",
        "        self.d, self.b = torch.symeig(self.c, eigenvectors=True)\n",
        "        self.d = self.d.sqrt().diag_embed()\n",
        "        self.gen_count += 1\n",
        "\n",
        "        info = {'step_size': self.step_size, 'correlation': correlation,\n",
        "                'fitness_max': max(f), 'fitness_mean': np.mean(f), 'c_norm': self.c.norm(),\n",
        "                'max_eigv': self.d.max()}\n",
        "        return ranked_results, info"
      ],
      "metadata": {
        "id": "YdezorLG0BOX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cma_es = CMA_ES(5, samples= 50, oversample= 0.5)\n",
        "\n",
        "for i in range(1000):\n",
        "    ranked_results, info= cma_es.step(objective_f)\n",
        "    print(info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Nk1EiA6QyL",
        "outputId": "0cadef2c-24d3-4ecf-c2fe-3137557d56e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'step_size': tensor(0.4454), 'correlation': tensor(0.5476), 'fitness_max': 538, 'fitness_mean': 18.26, 'c_norm': tensor(2.8617), 'max_eigv': tensor(1.4884)}\n",
            "{'step_size': tensor(0.3938), 'correlation': tensor(0.5188), 'fitness_max': 310, 'fitness_mean': 28.04, 'c_norm': tensor(3.5355), 'max_eigv': tensor(1.7580)}\n",
            "{'step_size': tensor(0.3426), 'correlation': tensor(0.4551), 'fitness_max': 575, 'fitness_mean': 65.62, 'c_norm': tensor(4.0270), 'max_eigv': tensor(1.9242)}\n",
            "{'step_size': tensor(0.2956), 'correlation': tensor(0.4227), 'fitness_max': 1126, 'fitness_mean': 139.42, 'c_norm': tensor(4.7799), 'max_eigv': tensor(2.1408)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DkFiVXgAOcGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}